
#+TITLE: APC523 -- Lecture Notes
#+AUTHOR: Jacob Levine

* Lecture 1 -- <2022-01-24 Sun> Introduction and Overview

** Defining computational science

- Theory -> Experiment (computation required for all steps)
- What is computational science?
  - concerned with the design, implementation, and use of mathematical models to analyse and solve scientific problems.
  - _Examples_: High Performance Computing, Numerical Analysis, applied to an Application domain (physics, climate, chemistry, ... ecology?)
  - We have a model for which we know the master equations -- we compute predictions by solving these equations.
  - vs. data science, where we have a lot of data but no equations (typically no theory)
- Using data science for scientific computing:
  - emulation techniques (interpolations in parameter space)
  - neural networks for subgrid turbulence models
  - machine learning for numerical regularization
- Software engineering vs. algorithmic design
- Simplify too much with pen+paper math (need to simulate to understand?)

** Scientific computing and experiments

- virtual experiments are cheaper than real experiments
  - explore in advance the parameter space
  - experiments are only used to calibrate a few critical cases
  - examples: nuclear testing, wind tunnels
  - Some experiments are just too complex -- forward modeling and mock observations

** Supercomputers

- exponential growth in supercomputer performance (moores (?) law)
- Computer architecture -- now in era of GPUs

** Algorithms

- Example from astrophysics:
  - N body codes (each galaxy is one body)
  - Direct N body: perform direct summation above for all particle \(O(N^2)\)
  - Particle mesh (PM or PIC): deposit particle mass onto a mesh and solve Poisson equation with the Fast Fourier Transform, Interpolate force back to particle. \(O(N lnN)\)
  - Tree code: group distant source particles into macro particles using an octree and a clever tree opening criterion \(O(N ln N)\)
  - PM with adaptive mesh refinement and multigrid method \(O(N)\)
  - Tree code with Fast Multipole Method: group also sink particles sharing the same distant force contribution (double tree walk) \(O(N)\)
- *Algorithms can lead to order of magnitude improvements in computing time*
- Can lead to arms race between developing fast algorithms and fast computers (need to adapt to new hardware!)

** Typical equations in physics and engineering

- Advection equation: \(\frac{du}{dt} + a\frac{du}{dx} = 0\)

    *Solution:*
    We use fourier analysis to solve this equation: \(u(x, t) = \int_{-\infty}^{infty}u_{hat}(k,t)e^{ikx}dk\)

    Since the advection equation is linear, we can solve each mode independently

    \(\frac{du}{dt} + ikau_{hat} ->\)

    Solution: \u(x,t) = (u_0(x-at)\).

- Wave equation: \(\frac{d^2u}{dt^2} = a^2\frac{d^2u}{dx^2}\)

  *Solution*

    Fourier transform again (review later)

    Homework: find u_0^+(x) and u_0^-(x) as a function of u(x,0) and \(\frac{du}{dt}(x,0)\)

- Heat equation: \(\frac{du}{dt} = v\frac{d^2}{dx^2}\)

    Fourier transform

- Burgers' equation: \(\frac{du}{dt} + u\frac{du}{dx} = v\frac{d^2u}{dx^2}\)
- All are Scalar 1D linear and non-linear Partial Differential Equations

- *More complex -- hyperbolic systems of conservation laws*

  - Euler equations -- system of scalar equations
    - Linearize to the linearized euler equations by defining small perturbations around an equilibrium.
    - calculate eigenvectors (for homework)


* COMMENT Lecture 2 -- <2022-01-26 Wed> Computing infrastructure

- Connecting to Adroit using =ssh=
- Software environment using =module=
- Software versioning using =git=

- adroit.princeton.edu
  - for Mac OS:
    - iTerm
    - XQuartz
- connect to adroit
  - off campus, connect to VPN
  - use =ssh -X yourlogin@adroit.princeton.edu=
  - enter password again
- get account on adroit

- gnuplot
  =plot "run.log" ::::1000 u 2:3 title "initial density"=
  - can I use R/python to generate my plots?

- Compile and run with mpi
  - =make <program> MPI=1=

- slurm
  - =srun -n <cores> t 00:01:00 <program name>=


* Lecture 4 -- <2022-02-02 Wed> Parallel computing

- Processor speed is plateauing.
- # cores per machine is increasing exponentially --> parallel computing is best way to increase computing speed
- Amdahl's law: theoreticla max speedup obtained by parallelizing a code ideally is given by\(Speedup(N) = \frac{T_s}{T_p(N)} = \frac{1}{\alpha + \frac{1 - \alpha}{N}}\)
  - \(\alpha\) is fraction of code that is not paralellizable, \(N\) is number of processors
- efficiency plateaus at \(1-\alpha\)
- Strong vs. weak scaling -- need to run both tests
- Parallel programming languages:
  - MPI (focus on this in course)
  - OpenMP (simpler than MPI -- but you control less)
  - GPU programming in CUDA or OpenACC/OpenMP
- Distributed vs. shared memory
  - MPI uses a distributed memory paradigm -- data are transferred explicitly between nodes through the network.

** OpenMP
- started in the 90s where multiple vendors were using their own multi-threading directives during the era of vector processing.
- On <1997-10-28 Tue> they all met and adopted the open multi processing standard
- OpenMP is specified by the architecture review board

*** General concepts
- multi-threading
- an openMP program is executed by only one process, called the master thread. The corresponding piece of code is called a sequential region.
- the master thread activates light-weight processes, called the worker threads. this marks in the code the entry of a parallel region.
- each thread executes a task corresponding to a block of instructions. During the execution of the task, variables can be read from or updated in memory.
- The variable can be defined in the local memory of the thread called the stack memory. The variable is called a private variable.
- The variable can be defined in the main shared (RAM) memory also called the heap. the variable is called a shared variable.
- *This can be a problem in openMP* -- distinction between shared and private variables not always explicitly
*** Compilation
- compilation directives and clauses:
  - they are put in the program to create the threads, define the work etc. need to specify -openmp on the compilation command linear
- Functions and routines:
  - openMP contains several dedicated functions like MPI. part of the library that can be linked at a link time.
- Environment variables:
  - OpenMP has several environmental variables that can be set at execution time.

*** Parallel region
- In a parallel region, by default, the data sharing attribute of variables is shared.
- within a single parallel region, all concurrent threads execute the same code in parallel.
- there is an implicit synchronization barrier at the end of the parallel region.

#+BEGIN_SRC fortran

program parallel
      !$ use OMP_LIB
      implicit none
      real :: a
      logical :: p

            a = 92290 ; pa=false.
            !SOMP_PARALLEL

#+END_SRC


- using the DEFAULT clause, it is possible to change the default attribute to PRIVATE
- if a variable is PRIVATE, it will be stored in the stack memory of each thread. Its value is undetermined when entering the parallel region.
- using the FIRSTPRIVATE clause, it is possible to force the initialization of a PRIVATE variable to the last value it has outside the parallel region.
- Using the NUM_THREADS() clause allows to set the number of children in a parallel region.
- OPT_GET_NUM_THREADS() gets current value

*** Parallel loops
- A parallel loop is a do-loop where each iteration is independent from the others.
- Loop indices are always private integer variables
- While loops are not supported.

*** Work scheduling
- the distribution strategy is set by a SCHEDULE directive
- proper scheduling can optimize the load-balancing of the work.
- By default, global syncronization is enacted at the end of each loop
- the NOWAIT directive can remove barrier
- can define schedule by RUNTIME

*Want to minimize the number of parallel regions* -- takes time to activate and kill threads. Multiple DO loops

*** Misc.
- In a parallel loop, if one needs to perform a global operation, one uses the REDUCTION clause.
- Logical -- AND OR EQV NEQV Intrinsic -- MAX MIN IAND IOR IEOR, Arithmetic
- Each thread computes partial results, which are combined at the end of the parallel loop.
- the directive =!$OMP BARRIER= forces teh synchronization of all threads within a parallel region
- The directive =ATOMIC= and =CRITICAL= can be used to force a serial variable update and avoid race conditions.

*** Best practices:
- minimize number of parallel regions
- adjust the number of threads to the size of the problem (threads come with overhead)
- always parallelize the outermost loop on the outermost moving index of an array (non-consecutive in memory)
- Conflicts between threads can lead to poor cache memory management (the so-called cache misses). L! and L2 memory management is key
- Performance analysis with OpenMP can be done using several functions to get wall clock time.

** MPI

*** Implementations
- OpenMPI - MPICH
*** Tools
- Debuggers and performance analysis tools:
  - Totalview, DDT, Scalasca (or use "write")
- Scientific libraries
  - Scalapack
  - PETSc
  - FFTW
*** General concepts
- MPI is a library which allows process coordination and scheduling between millions of processors using a message-passing paradigm.
- The message is sent from a source process to a target process: sender address and recipient address
- The message contains a header
  - Identifier of the sending process (sender id)
  - The type of the message data (datatype)
  - The length of the message data (data length)
  - Identifier of the receiving process (receiver id)
- The message contains data
- The messages are managed and interpreted by a runtime system comparable to a telephone provider or postal company
- Message are sent to a specific address. Receiving processes must be able to classify and interpret incoming messages
- An MPI application is a group of autonomous processes deployed on different nodes, each one executing its own code and communication to the other processes via calls to routines in the MPI library.

*** Data distribution
- Data are distributed between nodes and cores using a domain decomposition strategy (recursive bisection?)

*** Basics
- MPI environment variables: =include mpi.h=
- Launching the MPI environment
  - =MPI-INIT()= routine
- Terminating

*** Blocking send and receive




* Lecture <2022-02-09 Wed>
** Finite difference approximation
- basic method to design numerical approx of the first derivative is to use lagrange interpolation over a subset of points called the kernel
- In the general the kernel is defined such that \(x_i\) is in the center.
- If L=R, the kernel is symmetric and the FD scheme is called a central scheme
- If L>R the kernel is skewed left and called a backward or left-biased scheme (leading order term has negative sign)
- If L<R -- forward/right-biased scheme (leading order term has a positive sign)
- Leading order term is term in T-exp with power >
- =import sympy= -- for symbolic arithmetic in =python=.
